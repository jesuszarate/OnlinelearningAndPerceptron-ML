\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}

\newcommand{\semester}{Spring 2018}
\newcommand{\assignmentId}{2}
\newcommand{\releaseDate}{19 January, 2018}
\newcommand{\dueDate}{5 March, 2018}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learining \semester}
\author{Homework \assignmentId}
\date{Handed out: \releaseDate\\
  Due date: \dueDate}

\begin{document}
\maketitle

\input{emacscomm}
\input{general-instructions}


\section{Expressiveness of Linear Classifiers}
\begin{enumerate}
\item~[60 points] Can you figure out an equivalent linear classifier for the following boolean functions? Please point out what the weight vector is, what the bias parameter is, and what the hyperplane is. Note that the hyperplane is determined by an equality. If you cannot find out such a  linear classifier, please explain why. 
\begin{enumerate}
% a
\item $f(x_1, x_2, x_3) = x_1 \lor x_2 \lor x_3$\\
    $x_1 + x_2 + x_3 \geq 1$\\

    $\text{Hyperplane = } x_1 + x_2 + x_3 - 1 = 0$\\
    $\text{w = } [1, 1, 1]$\\
    $\text{b = } -1$

% b
\item $f(x_1, x_2, x_3) = x_1 \land \neg x_2 \land \neg x_3$\\
    $x_1 + (1 - x_{2}) + (1 - x_{3}) \geq 3$\\
    $x_1  - x_{2} - x_{3} + 2 \geq 3$\\

    $\text{Hyperplane = } x_1 - x_{2} - x_{3} - 1 = 0$\\
    $\text{w = } [1, -1, -1]$\\
    $\text{b = } -1$
%c
\item $f(x_1, x_2, x_3) = \neg x_1 \lor \neg x_2 \lor \neg x_3$\\
    $ (1 - x_{1}) + (1 - x_2) + (1 - x_3) \geq 1$\\
    $ -x_{1} - x_2 - x_3 + 3 \geq 1$\\

    $\text{Hyperplane = } -x_{1} - x_2 - x_3 + 2 = 0$\\
    $\text{w = } [-1, -1, -1]$\\
    $\text{b = } 2$

%d
\item $f(x_1, x_2, \ldots, x_n) = x_1 \lor x_2 \ldots \lor x_k$ (note that  $k <n$).\\
    $ x_1 + x_2 \ldots + x_k \geq 1$\\
    $\text{Hyperplane = } x_1 + x_2 \ldots + x_k - 1= 0$
    $\text{w = } [1, 1, 1 \dots 1, 1]$\\
    $\text{b = } -1$
%e
\item $f(x_1, x_2, x_3, x_4) = (x_1 \lor x_2) \land (x_3 \lor x_4)$\\

$(x_1 \lor x_2) \land (x_3 \lor x_4) = $
$(x_1 + x_2) >= 1$\\
$x_1 + x_2 - 1 = 0$\\

$(x_3 + x_4) >= 1$\\
$x_3 + x_4 - 1 = 0$\\

$(x_1 + x_2 - 1 + x_3 + x_4 - 1) >= 4$\\
$(x_1 + x_2 - 1 + x_3 + x_4 - 1) - 4 = 0$\\
$(x_1 + x_2 + x_3 + x_4) - 6 = 0$\\


\item $f(x_1, x_2, x_3, x_4) = (x_1 \land x_2) \lor (x_3 \land x_4)$

$(x_1 \land x_2) \lor (x_3 \land x_4)$
$(x_1 + x_2) >= 2$\\
$(x_3 + x_4) >= 2$\\

$(x_1 + x_2) - 2 = 0$\\
$(x_3 + x_4) - 2 = 0$\\

$(x_1 + x_2 - 2) + (x_3 + x_4 - 2) >= 1$\\
$(x_1 + x_2 - 2) + (x_3 + x_4 - 2) - 4 - 1 = 0$\\
$(x_1 + x_2) + (x_3 + x_4) - 5 = 0$\\


\end{enumerate}

\item~[50 points] Can you draw  equivalent decision trees for the following boolean functions? Note that you do NOT need to run the ID3 algorithm to learn such a tree. You only need to brainstorm and draw one. If you cannot, please explain why. 
\begin{enumerate}
\item $f(x_1, x_2, x_3) = x_1 \lor x_2 \lor x_3$

\includegraphics[scale=0.5]{Images/2-aTree.png}\\

\item $f(x_1, x_2, x_3) = x_1 \land \neg x_2 \land \neg x_3$

\includegraphics[scale=0.5]{Images/2-bTree.png}\\

\item $f(x_1, x_2, x_3) = \neg x_1 \lor \neg x_2 \lor \neg x_3$

\includegraphics[scale=0.5]{Images/2-cTree.png}\\

\item $f(x_1, x_2, x_3, x_4) = (x_1 \lor x_2) \land (x_3 \lor x_4)$

\includegraphics[scale=0.5]{Images/2-dTree.png}\\

\item $f(x_1, x_2, x_3, x_4) = (x_1 \land x_2) \lor (x_3 \land x_4)$

\includegraphics[scale=0.5]{Images/2-eTree.png}

\end{enumerate}

\item~[10 points] What do you conclude about the expressiveness of decision trees and linear classifiers from Problem 1 and 2? Why? 

Decision trees are more expressive than linear classifiers. With the linear classifiers we were not able to get two
of the boolean functions, but with the decision trees we were able to come up with
a tree for all of them.\\

\item~[30 points] The following boolean functions cannot be represented by linear classifiers. Can you work out some feature mapping such that, after mapping all the inputs of these functions into a higher dimensional space, you can easily identify a hyperplane that separates the inputs with different corresponding boolean function values? Please write down the separating hyperplane as well. 
\begin{enumerate}
\item $f(x_1, x_2) = (x_1 \land \neg x_2) \lor (\neg x_1 \land x_2) $
\item $f(x_1, x_2) = (x_1 \land x_2) \lor (\neg x_1 \land \neg x_2)$
\item $f(x_1, x_2, x_3)$ is listed in the following table

        \centering
        \begin{tabular}{ccc|c}
        $x_1$ & $x_2$ & $x_3$ &  $f(x_1, x_2, x_3)$\\ 
        \hline\hline
         0 & 0 & 0 & 0 \\ \hline
         0 & 0 & 1 & 1 \\ \hline
         0 & 1 & 0 & 1 \\ \hline
         1 & 0 & 0 & 1 \\ \hline
         0 & 1 & 1 & 0\\ \hline
         1 & 1 & 0 & 0\\ \hline
         1 & 0 & 1 & 0\\ \hline
         1 & 1 & 1 & 1\\ \hline
        \end{tabular}

\end{enumerate}

\item~[40 points] ~[\textbf{For 6350 students}] Given two vectors $\x = [x_1,  x_2]$ and $\y=[y_1,  y_2]$, find a feature mapping $\phi(\cdot)$ for each of the following functions, such that the function is equal to the inner product between the mapped feature vectors, $\phi(\x)$ and $\phi(\y)$. For example, $(\x^\top \y)^0 = \phi(\x)^\top \phi(\y)$ where $\phi(\x) = [1]$ and $\phi(\y) = [1]$; $(\x^\top \y)^1 = \phi(\x)^\top \phi(\y)$ where $\phi(\x) = \x$ and $\phi(\y) = \y$. 
\begin{enumerate}
\item~[10 points] $(\x^\top \y)^2$

    $= [x_1, x_2]^\top[y_1, y_2]$\\
    $= (x_1y_1 + x_2y_2)^2$\\
    $= (x_1y_1 + x_2y_2)(x_1y_1 + x_2y_2) $\\
    $= x_1 y_1 + 2x_1 y_1 x_2 y_2 + x_2 y_2 ^2$\\


$\phi(\x) = (\x^{\top}\y)^1$\\
$\phi(\y) = (\x^{\top}\y)^1$

\item~[10 points] $(\x^\top \y)^3$
    $= ([x_1, x_2]^\top[y_1, y_2])^3$\\
    $= (x_1y_1 + x_2y_2)^3$\\
    $= (x_1y_1 + x_2y_2)(x_1y_1 + x_2y_2)(x_1y_1 + x_2y_2) $\\
    $= (x_1y_1 + x_2y_2)^2(x_1y_1 + x_2y_2) $\\


    $\phi(\x) = (\x^{\top}\y)^2$\\
    $\phi(\y) = (\x^{\top}\y)^1$

\item~[20 points] $(\x^\top \y)^k$ where $k$ is  any positive integer.

    $= ([x_1, x_2]^\top[y_1, y_2])^k$\\
    $= (x_1y_1 + x_2y_2)^k$\\
    $= (x_1y_1 + x_2y_2)^{k-1}(x_1y_1 + x_2y_2) $\\

    $\phi(\x) = (\x^{\top}\y)^{k-1}$\\
    $\phi(\y) = (\x^{\top}\y)^1$

\end{enumerate}
\end{enumerate}

\section{Linear Regression}
Suppose we have the training data shown in Table \ref{tb:1}, from which we want to learn a linear regression model, parameterized by a weight vector $\w$ and a bias parameter $b$.  
\begin{table}
        \centering
        \begin{tabular}{ccc|c}
        $x_1 $ & $x_2$ & $x_3$ &  $y$\\ 
        \hline\hline
         1 & -1 & 2 & 1 \\ \hline
         1 & 1 & 3 & 4 \\ \hline
         -1 & 1 & 0 & -1 \\ \hline
         1 & 2 & -4 & -2 \\ \hline
         3 & -1 & -1 & 0\\ \hline
         \end{tabular}
         \caption{Linear regression training data.}\label{tb:1}
\end{table}

\begin{enumerate}
\item~[10 points] Write down the LMS (least mean square) cost function $J(\w, b)$.

    $J(\w, b) = \frac{1}{2} \sum_{i=1}^m(y_i - \w^\top\x_{i} - b)^2$

\item~[30 points] Calculate the gradient $\frac{\nabla J}{\nabla \w}$ and $\frac{\nabla J}{\nabla b}$
\[
    \frac{\nabla J}{\nabla \w} = - \sum^m_{i=1}(y_i - \w^\top x_i - b)x_i
\]

\[
    \frac{\nabla J}{\nabla b} = - \sum^m_{i=1}(y_i - \w^\top x_i - b)
\]
\item \begin{enumerate}


\item when $\w = [0,0,0]^\top$ and $b = 0$;


%\begin{align*}
%    \frac{\nabla J}{\nabla \w} &= -(1 - \w^\top \x_1)x_{1,1} \\
%    &+ (1 - \w^\top \x_1)x_{1,1} \\
%    &+ (1 - \w^\top \x_1)x_{1,1} \\
%    &+ (1 - \w^\top \x_1)x_{1,1} \\
%    &+ (1 - \w^\top \x_1)x_{1,1}
%\end{align*}\item

    $\w^\top \x_1 = [0, 0, 0]^\top[1, -1, 2] = 0$ \\
    $\w^\top \x_2 = [0, 0, 0]^\top[1, 1, 3] = 0$  \\
    $\w^\top \x_3 = [0, 0, 0]^\top[-1, 1, 0] = 0$ \\
    $\w^\top \x_4 = [0, 0, 0]^\top[1, 2, -4] = 0$ \\
    $\w^\top \x_5 = [0, 0, 0]^\top[3, -1, -1] = 0$ \\

\begin{align*}
    \frac{\nabla J}{\nabla \w} &=
    (1 - 0.0 - 0) * [ 1, -1,  2]\\
    &+ (4 - 0.0 - 0) * [1, 1, 3]\\
    &+ (-1 - 0.0 - 0) * [-1,  1,  0]\\
    &+ (-2 - 0.0 - 0) * [ 1,  2, -4]\\
    &+ (0 - 0.0 - 0) * [ 3, -1, -1]\\
    &= [ -4,   2, -22]
\end{align*}

\begin{align*}
    \frac{\nabla J}{\nabla b} &=
    (1 - 0.0 - 0)\\
    &+ (4 - 0.0 - 0)\\
    &+ (-1 - 0.0 - 0)\\
    &+ (-2 - 0.0 - 0)\\
    &+ (0 - 0.0 - 0)\\
    &= [-2.]
\end{align*}\item

%$\frac{\nabla J}{\nabla \w_1}
%= -(1 - 0)1
%+ (4 - 0)1
%+ (-1 - 0)-1
%+ (-2 - 0)1
%+ (0 - 0)3
%= -1$
%
%
%
%$\frac{\nabla J}{\nabla \w_2}
%= -(1 - 0)-1
%+ (4 - 0)1
%+ (-1 - 0)1
%+ (-2 - 0)2
%+ (0 - 0)-1
%= -4$
%
%
%
%$\frac{\nabla J}{\nabla \w_3}
%= -(1 - 0)2
%+ (4 - 0)3
%+ (-1 - 0)0
%+ (-2 - 0)-4
%+ (0 - 0)-1
%= 3$



\item when $\w = [-1,1,-1]^\top$ and $b = -1$;

$\w^\top \x_1 = [-1, 1, -1]^\top[1, -1, 2]  = -4$\\
$\w^\top \x_2 = [-1, 1, -1]^\top[1, 1, 3]   = -3$\\
$\w^\top \x_3 = [-1, 1, -1]^\top[-1, 1, 0]  = 2 $\\
$\w^\top \x_4 = [-1, 1, -1]^\top[1, 2, -4]  = 5 $\\
$\w^\top \x_5 = [-1, 1, -1]^\top[3, -1, -1] = -3$ \\

\begin{align*}
    \frac{\nabla J}{\nabla \w} &=
    (1 + 4 + 1) * [1,-1,2]\\
    &+ (4 + 3 + 1) * [1,1,3]\\
    &+ (-1 - 2 + 1) * [-1,1,0]\\
    &+ (-2 - 5 + 1) * [1,2,-4]\\
    &+ (0 + 3 + 1) * [3,-1,-1]\\
    &= [-22,  16, -56]
\end{align*}

\begin{align*}
    \frac{\nabla J}{\nabla b} &=
    (1 + 4 + 1)\\
    &+ (4 + 3 + 1)\\
    &+ (-1 - 2 + 1)\\
    &+ (-2 - 5 + 1)\\
    &+ (0 + 3 + 1)\\
    &= -10 % Might not be correct
\end{align*}

%\begin{align*}
%    \frac{\nabla J}{\nabla \w_1}
%    &= -((1 + 4)1 \\
%    &+  ( 4 + 3)1 \\
%    &+  ( -1- 2 )-1 \\
%    &+  ( -2- 5 )1 \\
%    &+  ( 0 + 3)3)\\
%    &= -1
%\end{align*}

%$\frac{\nabla J}{\nabla \w_1}
%= -((1 + 4)1
%+  ( 4 + 3)1
%+  ( -1- 2 )-1
%+  ( -2- 5 )1
%+  ( 0 + 3)3)
%= -17$

%\begin{align*}
%    \frac{\nabla J}{\nabla \w_1}
%    &= -(1 - 0)-1 \\
%    &+ (4 - 0)1 \\
%    &+ (-1 - 0)1 \\
%    &+ (-2 - 0)2 \\
%    &+ (0 - 0)-1 \\
%    &= -4
%\end{align*}

%$\frac{\nabla J}{\nabla \w_2}
%= -((1  + 4)-1
%+   (4  + 3)1
%+   (-1 - 2 )1
%+   (-2 - 5 )2
%+   (0  + 3)-1)
%= 4$


%\begin{align*}
%    \frac{\nabla J}{\nabla \w_1}
%    &= -(1 - 0)2 \\
%    &+ (4 - 0)3 \\
%    &+ (-1 - 0)0 \\
%    &+ (-2 - 0)-4 \\
%    &+ (0 - 0)-1 \\
%    &= 3
%\end{align*}

%$\frac{\nabla J}{\nabla \w_3}
%= -((1  + 4) 2
%+   (4  + 3) 3
%+   (-1 - 2) 0
%+   (-2 - 5)-4
%+   (0  + 3)-1)
%= -22$ \\



\item when $\w = [1/2,-1/2,1/2]^\top$ and $b = 1$.

$\w^\top \x_1 = [1/2, -1/2, 1/2]^\top[1, -1, 2]  = 2   $     \\
$\w^\top \x_2 = [1/2, -1/2, 1/2]^\top[1, 1, 3]   = 3/2 $    \\
$\w^\top \x_3 = [1/2, -1/2, 1/2]^\top[-1, 1, 0]  = -1  $    \\
$\w^\top \x_4 = [1/2, -1/2, 1/2]^\top[1, 2, -4]  = -5/2$  \\
$\w^\top \x_5 = [1/2, -1/2, 1/2]^\top[3, -1, -1] = 3/2 $  \\

\begin{align*}
    \frac{\nabla J}{\nabla \w} &=
    (1 - 2.0 - 1) * [1,-1,2]\\
    &+ (4 - 1.5 - 1) * [1,1,3]\\
    &+ (-1 + 1.0 - 1) * [-1,1,0]\\
    &+ (-2 + 2.5 - 1) * [1,2,-4]\\
    &+ (0 - 1.5 - 1) * [3,-1,-1]\\
    &= [ 7.5 -4  -5 ]
\end{align*}

\begin{align*}
    \frac{\nabla J}{\nabla b} &=
    (1 - 2.0 - 1)\\
    &+ (4 - 1.5 - 1)\\
    &+ (-1 + 1.0 - 1)\\
    &+ (-2 + 2.5 - 1)\\
    &+ (0 - 1.5 - 1)\\
    &= 4.5 %might not be right
\end{align*}

%$\frac{\nabla J}{\nabla \w_1}
%= -((1 + 2   )1
%+  ( 4 + 3/2 )1
%+  ( -1- -1  )-1
%+  ( -2- -5/2)1
%+  ( 0 + 3/2 )3)
%= -25/2$
%
%$\frac{\nabla J}{\nabla \w_2}
%= -((1  + 2   )-1
%+   (4  + 3/2 )1
%+   (-1 - -1   )1
%+   (-2 - -5/2 )2
%+   (0  + 3/2 )-1)
%= -9$
%
%$\frac{\nabla J}{\nabla \w_3}
%= -((1  + 2   ) 2
%+   (4  + 3/2 ) 3
%+   (-1 - -1  ) 0
%+   (-2 - -5/2)-4
%+   (0  + 3/2 )-1)
%= -39/2$ \\



\end{enumerate}
\item~[20 points] What are the optimal $\w$ and $\b$ that minimize the cost function?

    $\w = [0,0,0]$\\
    $\b = 0$\\\\
    $\w = [-1,1,-1]$\\
    $\b = -1$\\\\
    $\w = [1/2,-1/2,1/2]$\\
    $\b = 1$\\\\

\item~[50 points] Now, we want to use stochastic gradient descent to minimize $J(\w, b)$, we initialize $\w = \0$ and $b = 0$. We set the learning rate $r = 0.1$ and sequentially go through the $5$ training examples. Please list the stochastic gradient in each step and the updated $\w$ and $b$.
$i = 1$\\
$w\big[0\big] = 0.0000 + 0.1*(1 - 0.0000 - 0.0000)*1$\\
$w\big[1\big] = 0.0000 + 0.1*(1 - 0.0000 - 0.0000)*-1$\\
$w\big[2\big] = 0.0000 + 0.1*(1 - 0.0000 - 0.0000)*2$\\
$\w = [ 0.1 -0.1  0.2]$\\

$b = 0.0000 + 0.1*(1.0000 - 0.0000 - 0.0000)$\\
$b = 0.1000$\\

$i = 2$\\
$w\big[0\big] = 0.1000 + 0.1*(4 - 0.6000 - 0.1000)*1$\\
$w\big[1\big] = -0.1000 + 0.1*(4 - 0.6000 - 0.1000)*1$\\
$w\big[2\big] = 0.2000 + 0.1*(4 - 0.6000 - 0.1000)*3$\\
$\w = [ 0.43  0.23  1.19]$\\

$b = 0.1000 + 0.1*(4.0000 - 0.6000 - 0.1000)$\\
$b = 0.4300$\\\\

$i = 3$\\
$w\big[0\big] = 0.4300 + 0.1*(-1 + 0.2000 - 0.4300)*-1$\\
$w\big[1\big] = 0.2300 + 0.1*(-1 + 0.2000 - 0.4300)*1$\\
$w\big[2\big] = 1.1900 + 0.1*(-1 + 0.2000 - 0.4300)*0$\\
$\w = [ 0.553  0.107  1.19 ]$\\

$b = 0.4300 + 0.1*(-1.0000 - -0.2000 - 0.4300)$\\
$b = 0.3070$\\\\

$i = 4$\\
$w\big[0\big] = 0.5530 + 0.1*(-2 + 3.9930 - 0.3070)*1$\\
$w\big[1\big] = 0.1070 + 0.1*(-2 + 3.9930 - 0.3070)*2$\\
$w\big[2\big] = 1.1900 + 0.1*(-2 + 3.9930 - 0.3070)*-4$\\
$\w = [ 0.7216  0.4442  0.5156]$\\

$b = 0.3070 + 0.1*(-2.0000 - -3.9930 - 0.3070)$\\
$b = 0.4756$\\\\

$i = 5$\\
$w\big[0\big] = 0.7216 + 0.1*(0 - 1.2050 - 0.4756)*3$\\
$w\big[1\big] = 0.4442 + 0.1*(0 - 1.2050 - 0.4756)*-1$\\
$w\big[2\big] = 0.5156 + 0.1*(0 - 1.2050 - 0.4756)*-1$\\
$\w = [ 0.21742,  0.61226,  0.68366]$\\

$b = 0.4756 + 0.1*(0.0000 - 1.2050 - 0.4756)$\\
$b = 0.3075$\\

\end{enumerate}

\section{Mistake Driven Learning Algorithm}
Identify the maximum number of mistakes made by Halving algorithm in learning a target function from following concept classes. Please check whether the Halving algorithm is a mistake bound algorithm.
\begin{enumerate}
\item~[10 points] Disjunction of $n$ boolean variables.

    $f(x) = x_1 \lor x_2 \lor \dots \lor x_n$

\item~[10 points] Disjunction of $k$ boolean variables out of the total $n$ input variables. Note $k$ is a constant and smaller than $n$.
\item~[10 points] $m$-of-$n$ rules. Note $m$ is a constant and smaller than $n$.
\item~[20 points] All boolean function of $n$ input boolean variables.
\end{enumerate}

\section{Perceptron}
\begin{enumerate}
\item  Let us review the Mistake Bound Theorem discussed in our lecture.
\begin{enumerate}
	\item~[10 points] If we change the second assumption to be as follows: Suppose there exists a vector $\u\in \mathbb{R}^n$, and a positive $\gamma$, we have for each $(\x_i, y_i)$ in the training data, $y_i(\u^\top \x_i) \ge \gamma$. What is the upper bound for the number of mistakes made by the Perceptron algorithm?   Note that $\u$ is unnecessary to be a unit vector.



	\item~[10 points] Following (a), if we do NOT assume $\u$ is a unit vector, and we still want to obtain the same upper bound introduced in the lecture, how should we change the inequalities in the second assumption?
	\item~[20 points]  Now, let us state the second assumption in another way: Suppose there a hyperplane that can correctly separate all the positive examples from the negative examples in the data, and the margin for this hyper plane is $\gamma$. What is the upper bound for the number of mistakes made by Perceptron algorithm?
\end{enumerate}

\item~[20 points] We want to use Perceptron to learn a disjunction as follows,
\[
f(x_1, x_2, \ldots, x_n) = \neg x_1 \lor \neg \ldots \neg x_k \lor x_{k+1} \lor \ldots \lor x_{2k} \;\;\;\;(\mathrm{note\; that}\;\; 2k < n).
\]
Please derive an upper bound of the number of mistakes made by Perceptron in learning this disjunction. Is Perceptron a mistake bound algorithm?
\end{enumerate}


\section{Programming Assignments}

\begin{enumerate}
\item We will implement the LMS method for a linear regression task. The dataset is from UCI repository (\url{https://archive.ics.uci.edu/ml/datasets/Concrete+Slump+Test}). The task is to predict the real-valued SLUMP of the concrete, with $7$ features. The features and output are listed in the file ``regression/data-desc.txt''. The training data are stored in the file ``regression/train.csv'', consisting of $53$ examples. The test data are stored in ``regression/test.csv'', and comprise of $50$ examples. In both the training and testing datasets, feature values and outputs are separated by commas.

\begin{enumerate}
\item~[90 points] Implement the batch gradient descent algorithm, and tune the learning rate $r$ to ensure the algorithm converges.  To examine convergence, you can watch the norm of the weight vector difference,  $\|w_{t} - w_{t-1}\|$,  at each step $t$.  if $\|w_{t} - w_{t-1}\|$ is  less than a tolerance level, say, $1e-6$, you can conclude that it converges. You can initialize your weight vector to be $\0$.  Please find an appropriate $r$ such that the algorithm converges. To tune $r$, you can start with a relatively big value, say, $r=1$, and then gradually decrease $r$, say $r=0.5, 0.25, 0.125, \ldots$, until you see the convergence. 
Report the learned weight vector, and the learning rate $r$. Meanwhile, please record the cost function  value of the training data at each step, and then draw a figure shows how the cost function changes along with steps. Use your final weight vector to calculate  the cost function value of the test data. 
%To do so, you can start $r$ to be relatively big, say, $r=1$, and then gradually decrease $r$. For a specific setting of $r$, you can calculate the cost function after each update and draw a curve showing how the cost function changes along with the number of updates. If you find the cost function on your curve tends to converge, you can conclude your algorithm convergences. 
\item~[90 points] Implement the stochastic gradient descent (SGD) algorithm. You can initialize your weight vector to be $\0$. Each step, you randomly sample a training example, and then calculate the stochastic gradient to update the weight vector.  Tune the learning rate $r$ to ensure your SGD converges. To check convergence, you can calculate the cost function of the training data after each stochastic gradient update, and draw a figure showing how the cost function values vary along with the number of updates. At the beginning, your curve will oscillate a lot. However, with an appropriate $r$, as more and more updates are finished, you will see the cost function tends to converge. Please report the learned weight vector, and the learning rate you chose, and the cost function value of the test data with your learned weight vector.   
\item~[20 points] We have discussed how to  calculate the optimal weight vector with an analytical form. Please calculate the optimal weight vector in this way. Comparing with the  weight vectors learned by batch gradient descent and stochastic gradient descent, what can you conclude? Why?
\end{enumerate}



\item We will implement  Perceptron for a binary classification task. The features and labels are listed in the file ``classification/data-desc.txt''. The training data are stored in the file ``classification/train.csv'', consisting of $872$ examples. The test data are stored in ``classification/test.csv'', and comprise of $500$ examples. In both the training and testing datasets, feature values and labels are separated by commas. 
\begin{enumerate}
\item~[60 points] Implement the standard Perceptron. Set the maximum number of epochs $T$ to 10. Report your learned weight vector, and the average prediction error on the test dataset. 
\item~[60 points] Implement the voted Perceptron. Set the maximum number of epochs $T$ to 10. Report the list of the distinct weight vectors and their counts --- the number of correctly predicted training examples. Using this set of weight vectors to predict each test example. Report the average test error. 
\item~[60 points] Implement the average Perceptron. Set the maximum number of epochs $T$ to 10. Report your learned weight vector. Comparing with the list of weight vectors from (b), what can you observe? Report the average prediction error on the test data. 
\item~[20 points] Compare the average prediction errors for the three methods. What do you conclude? 
\end{enumerate}

\end{enumerate}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
